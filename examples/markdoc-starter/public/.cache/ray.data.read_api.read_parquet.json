{
    "name": "ray.data.read_api.read_parquet",
    "signature": "ray.data.read_api.read_parquet(paths: Union[str, List[str]], *, filesystem: Optional[ForwardRef('pyarrow.fs.FileSystem')] = None, columns: Optional[List[str]] = None, parallelism: int = -1, ray_remote_args: Dict[str, Any] = None, tensor_column_schema: Optional[Dict[str, Tuple[numpy.dtype, Tuple[int, ...]]]] = None, meta_provider: Optional[ray.data.datasource.parquet_meta_provider.ParquetMetadataProvider] = None, partition_filter: Optional[ray.data.datasource.partitioning.PathPartitionFilter] = None, partitioning: Optional[ray.data.datasource.partitioning.Partitioning] = Partitioning(style='hive', base_dir='', field_names=None, field_types={}, filesystem=None), shuffle: Optional[Literal['files']] = None, include_paths: bool = False, file_extensions: Optional[List[str]] = None, concurrency: Optional[int] = None, override_num_blocks: Optional[int] = None, **arrow_parquet_args) -> ray.data.dataset.Dataset",
    "summary": "Creates a :class:`~ray.data.Dataset` from parquet files.",
    "desc": null,
    "args": [
        {
            "name": "paths",
            "type": null,
            "desc": "A single file path or directory, or a list of file paths. Multiple\ndirectories are not supported."
        },
        {
            "name": "filesystem",
            "type": null,
            "desc": "The PyArrow filesystem\nimplementation to read from. These filesystems are specified in the\n`pyarrow docs <https://arrow.apache.org/docs/python/api/            filesystems.html#filesystem-implementations>`_. Specify this parameter if\nyou need to provide specific configurations to the filesystem. By default,\nthe filesystem is automatically selected based on the scheme of the paths.\nFor example, if the path begins with ``s3://``, the ``S3FileSystem`` is\nused. If ``None``, this function uses a system-chosen implementation."
        },
        {
            "name": "columns",
            "type": null,
            "desc": "A list of column names to read. Only the specified columns are\nread during the file scan."
        },
        {
            "name": "parallelism",
            "type": null,
            "desc": "This argument is deprecated. Use ``override_num_blocks`` argument."
        },
        {
            "name": "ray_remote_args",
            "type": null,
            "desc": "kwargs passed to :meth:`~ray.remote` in the read tasks."
        },
        {
            "name": "tensor_column_schema",
            "type": null,
            "desc": "A dict of column name to PyArrow dtype and shape\nmappings for converting a Parquet column containing serialized\ntensors (ndarrays) as their elements to PyArrow tensors. This function\nassumes that the tensors are serialized in the raw\nNumPy array format in C-contiguous order (e.g., via\n`arr.tobytes()`)."
        },
        {
            "name": "meta_provider",
            "type": null,
            "desc": "A :ref:`file metadata provider <metadata_provider>`. Custom\nmetadata providers may be able to resolve file metadata more quickly and/or\naccurately. In most cases you do not need to set this parameter."
        },
        {
            "name": "partition_filter",
            "type": null,
            "desc": "A\n:class:`~ray.data.datasource.partitioning.PathPartitionFilter`. Use\nwith a custom callback to read only selected partitions of a dataset."
        },
        {
            "name": "partitioning",
            "type": null,
            "desc": "A :class:`~ray.data.datasource.partitioning.Partitioning` object\nthat describes how paths are organized. Defaults to HIVE partitioning."
        },
        {
            "name": "shuffle",
            "type": null,
            "desc": "If setting to \"files\", randomly shuffle input files order before read.\nDefaults to not shuffle with ``None``."
        },
        {
            "name": "arrow_parquet_args",
            "type": null,
            "desc": "Other parquet read options to pass to PyArrow. For the full\nset of arguments, see the `PyArrow API <https://arrow.apache.org/docs/                python/generated/pyarrow.dataset.Scanner.html                    #pyarrow.dataset.Scanner.from_fragment>`_"
        },
        {
            "name": "include_paths",
            "type": null,
            "desc": "If ``True``, include the path to each file. File paths are\nstored in the ``'path'`` column."
        },
        {
            "name": "file_extensions",
            "type": null,
            "desc": "A list of file extensions to filter files by."
        },
        {
            "name": "concurrency",
            "type": null,
            "desc": "The maximum number of Ray tasks to run concurrently. Set this\nto control number of tasks to run concurrently. This doesn't change the\ntotal number of tasks run or the total number of output blocks. By default,\nconcurrency is dynamically decided based on the available resources."
        },
        {
            "name": "override_num_blocks",
            "type": null,
            "desc": "Override the number of output blocks from all read tasks.\nBy default, the number of output blocks is dynamically decided based on\ninput data size and available resources. You shouldn't manually set this\nvalue in most cases."
        }
    ],
    "returns": ":class:`~ray.data.Dataset` producing records read from the specified parquet\nfiles.",
    "examples": [
        {
            "desc": null,
            "code": "Read a file in remote storage.\n\n>>> import ray\n>>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\")\n>>> ds.schema()\nColumn        Type\n------        ----\nsepal.length  double\nsepal.width   double\npetal.length  double\npetal.width   double\nvariety       string\n\nRead a directory in remote storage.\n\n>>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris-parquet/\")\n\nRead multiple local files.\n\n>>> ray.data.read_parquet(\n...    [\"local:///path/to/file1\", \"local:///path/to/file2\"]) # doctest: +SKIP\n\nSpecify a schema for the parquet file.\n\n>>> import pyarrow as pa\n>>> fields = [(\"sepal.length\", pa.float32()),\n...           (\"sepal.width\", pa.float32()),\n...           (\"petal.length\", pa.float32()),\n...           (\"petal.width\", pa.float32()),\n...           (\"variety\", pa.string())]\n>>> ds = ray.data.read_parquet(\"s3://anonymous@ray-example-data/iris.parquet\",\n...     schema=pa.schema(fields))\n>>> ds.schema()\nColumn        Type\n------        ----\nsepal.length  float\nsepal.width   float\npetal.length  float\npetal.width   float\nvariety       string\n\nThe Parquet reader also supports projection and filter pushdown, allowing column\nselection and row filtering to be pushed down to the file scan.\n\n.. testcode::\n\n    import pyarrow as pa\n\n    # Create a Dataset by reading a Parquet file, pushing column selection and\n    # row filtering down to the file scan.\n    ds = ray.data.read_parquet(\n        \"s3://anonymous@ray-example-data/iris.parquet\",\n        columns=[\"sepal.length\", \"variety\"],\n        filter=pa.dataset.field(\"sepal.length\") > 5.0,\n    )\n\n    ds.show(2)\n\n.. testoutput::\n\n    {'sepal.length': 5.1, 'variety': 'Setosa'}\n    {'sepal.length': 5.4, 'variety': 'Setosa'}\n\nFor further arguments you can pass to PyArrow as a keyword argument, see the\n`PyArrow API reference <https://arrow.apache.org/docs/python/generated/        pyarrow.dataset.Scanner.html#pyarrow.dataset.Scanner.from_fragment>`_."
        }
    ]
}