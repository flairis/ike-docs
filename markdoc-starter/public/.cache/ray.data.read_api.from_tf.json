{
    "name": "ray.data.read_api.from_tf",
    "signature": "ray.data.read_api.from_tf(dataset: 'tf.data.Dataset') -> ray.data.dataset.MaterializedDataset",
    "summary": "Create a :class:`~ray.data.Dataset` from a",
    "desc": "`TensorFlow Dataset <https://www.tensorflow.org/api_docs/python/tf/data/Dataset/>`_.\n\nThis function is inefficient. Use it to read small datasets or prototype.\n\n.. warning::\n    If your dataset is large, this function may execute slowly or raise an\n    out-of-memory error. To avoid issues, read the underyling data with a function\n    like :meth:`~ray.data.read_images`.\n\n.. note::\n    This function isn't parallelized. It loads the entire dataset into the local\n    node's memory before moving the data to the distributed object store.",
    "args": [
        {
            "name": "dataset",
            "type": null,
            "desc": "A `TensorFlow Dataset`_."
        }
    ],
    "returns": "A :class:`MaterializedDataset` that contains the samples stored in the `TensorFlow Dataset`_.",
    "examples": [
        {
            "desc": null,
            "code": ">>> import ray\n>>> import tensorflow_datasets as tfds\n>>> dataset, _ = tfds.load('cifar10', split=[\"train\", \"test\"])  # doctest: +SKIP\n>>> ds = ray.data.from_tf(dataset)  # doctest: +SKIP\n>>> ds  # doctest: +SKIP\nMaterializedDataset(\n    num_blocks=...,\n    num_rows=50000,\n    schema={\n        id: binary,\n        image: numpy.ndarray(shape=(32, 32, 3), dtype=uint8),\n        label: int64\n    }\n)\n>>> ds.take(1)  # doctest: +SKIP\n[{'id': b'train_16399', 'image': array([[[143,  96,  70],\n[141,  96,  72],\n[135,  93,  72],\n...,\n[ 96,  37,  19],\n[105,  42,  18],\n[104,  38,  20]],\n...,\n[[195, 161, 126],\n[187, 153, 123],\n[186, 151, 128],\n...,\n[212, 177, 147],\n[219, 185, 155],\n[221, 187, 157]]], dtype=uint8), 'label': 7}]"
        }
    ]
}